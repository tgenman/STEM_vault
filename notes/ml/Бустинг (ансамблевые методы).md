---
publish: true
---

Обучаем алгоритмы последовательно, каждый следующий уделяет особое внимание тем случаям, на которых ошибся предыдущий.

Как в беггинге, мы делаем выборки из исходных данных, но теперь не совсем случайно. В каждую новую выборку мы берём часть тех данных, на которых предыдущий алгоритм отработал неправильно. То есть как бы доучиваем новый алгоритм на ошибках предыдущего.

![[Pasted image 20230503215403.png]]


- Gradient Boosting
- AdaBoost
- Catboost
- XGBoost
- LightGBM

Сегодня есть три популярных метода бустинга, отличия которых хорошо донесены в статье [CatBoost vs. LightGBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)


#### Bookmarks
- [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)


**Connected with:**
- subtype of [[Ensemble Methods]]



created: 2023-05-03 21:49