---
aliases:
  - Natural Language Processing
publish: true
created: 2025-01-19 23:28
parent: "[[004.85 Machine Learning (notion)|ML]]"
connected: 
tags:
  - MOC
---

[[One-hot Encoding]]
[[Embeddings]]
[[Text Feature Representation]]


### Экзаменационные вопросы
1. [[Bag of Words (BoW)]], [[TF-IDF (Term Frequency-Inverse Document Frequency)|TF-IDF]]
2. [[Word embeddings. Word2vec CBOW, skip-gram, negative sampling. Word2vec properties.]]
3. [[CNNs in text processing]]
4. [[Seq2seq, encoder-decoder models]]
5. [[Attention mechanism in encoder-decoder models (as of Bahdanau et.al.)]]
6. [[Machine translation. Problem setup, training inference procedures. Quality metrics for MT.]]
7. [[Unsupervised translation approach]]
8. [[Transformer Architecture and Self-Attention Mechanism in Transformers]]
9. [[Transformer architecture blocks structure, layer norm, training procedure (adamW, lr scheduling, label smoothing)]]

10. BERT pretraining. MLM and NSP objectives.

11. BERT finetuning. Transfer learning paradigm. Finetuning strategies.

12. Discriminative transformer models (ROBERTA, ELECTRA, ALBERT)

13. Neural language modeling. Problem setup, key architectures (GPT-based or analogous).

Quality metrics for LM.

1. Transfer learning paradigm. T5, instruction-tuning (T0).

2. Generative pretraining. Large language models. Basic LLM adaptation (zero-shot, few-

shot prompting).

3. Transfer learning with LLMs. Finetuning strategies: full finetuning, p-tuning, adapters.

4. Learning from human feedback for LLMs. RLHF. PPO & DPOL

5. LLM alignment.

6. LLM Instruction tuning

7. Merge algorithms for LLM

8. Basic RAG.

9. Tools for LLM.
