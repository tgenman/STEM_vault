Давайте разберем, как матрица X связана с матрицей Z₀.

Z₀ - это расширенная версия матрицы X, которая включает в себя как признаки, так и отклики. Она имеет специальную структуру из двух строк блоков:

```
Z₀ = [x(1)   x(2)   ...   x(n)   x(n+1)]  # Верхний блок - признаки
     [y(1)   y(2)   ...   y(n)     0   ]  # Нижний блок - отклики
```

Размерность Z₀ равна (d+1)×(n+1), где:
- d+1 строк (d строк для признаков + 1 строка для откликов)
- n+1 столбцов (n примеров + 1 точка для предсказания)

Связь между X и Z₀ следующая:
1. Верхний блок Z₀ содержит транспонированную матрицу X. То есть если в X признаки были строками, то в Z₀ они становятся столбцами.
2. Нижняя строка Z₀ содержит соответствующие отклики y(i) = ⟨x(i),w*⟩ для каждого примера, а для последней точки x(n+1) стоит 0, так как это значение нужно предсказать.

Например, если у нас d=2 (два признака) и n=3 (три обучающих примера), то:

Матрица X будет размера 4×2:
```
X = [1.0  2.0]  # x(1)
    [3.0  4.0]  # x(2)
    [5.0  6.0]  # x(3)
    [7.0  8.0]  # x(n+1)
```

А соответствующая матрица Z₀ будет размера 3×4:
```
Z₀ = [1.0  3.0  5.0  7.0]  # Первый признак
     [2.0  4.0  6.0  8.0]  # Второй признак
     [y₁   y₂   y₃    0 ]  # Отклики
```

Такая структура Z₀ позволяет трансформеру одновременно видеть как признаки, так и отклики, что важно для задачи обучения в контексте.

---

Память передается между слоями внутри одного forward pass, что позволяет реализовать сложные методы оптимизации.

Убрана сеть FFN для упрощения вычислений
- ==Может просто добавить FFN и попробовать методы второго порядка?

Авторы показывают, что их архитектура может эффективно справляться с ill-conditioned problems, имитируя поведение CGD. Это важно, потому что многие задачи в машинном обучении являются плохо обусловленными, особенно при обучении глубоких нейронных сетей.