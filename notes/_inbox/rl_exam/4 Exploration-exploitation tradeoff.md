---
aliases: 
publish: true
anki: false
created: 2025-02-12 13:01
parent: 
connected:
  - "#обс/linking"
tags:
  - empty
---

### Exploration and Exploitation

- Reinforcement learning is like trial-and-error learning
- The agent should discover a good policy
- From its experiences of the environment
- Without losing too much reward along the way

- Exploration finds more information about the environment
- Exploitation exploits known information to maximise reward
- It is usually important to explore as well as exploit



The exploration-exploitation tradeoff: the conflict and balance between two goals:

Exploration:

- Gathering new information about the environment.

- The agent tries new actions or strategies to potentially find better solutions.

- Crucial for discovering optimal long-term strategies.

Exploitation:

- Using known information to maximize immediate rewards.

- The agent chooses actions known to yield good results.

- Important for obtaining high rewards in the short term.

  

If an agent only explores, it may miss opportunities for high rewards. If an agent only exploits, it may get stuck in a local optimum and miss better strategies.

  

Examples:

Multi-armed bandits: choosing between slot machines with unknown win probabilities.

RecSys: balancing between recommending known popular items and exploring new user preferences.

  

Balancing strategies:

- ε-greedy: Random action with probability ε, best known action otherwise

- Softmax: Action probability proportional to expected reward. Take softmax of Q(s, a) over all possible a. t - temperature that controls distribution

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc6ASv69fiwqzCzysyUTbuyZzJ_iaPwwVwK-RB8FxofhNBLvFpC19mKONuPZPcp7iGX_ZovTvcUYFDG4yyQExNWFE3PRKIB95mhjfuLF-302lu0YJ3Gv960Z88wsJKzXQ5bN4nv?key=bLVzrIeeji0xOX_OWgzfwxJh)

- Upper Confidence Bound (UCB): Considers potential and uncertainty

[https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/](https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/)

https://www.cs.cornell.edu/courses/cs6783/2021fa/lec25.pdf

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQ8psB_aJ_AlSdiuo-j3HGQs5fIIfnuTobXTEIcmTSADU-6ihOIYJzTyXmQXei-Xr-olSfWF19DSLPl5116vrR0ADn6bDuTRo-V7h-MUc_I8Im63YJzDn2FpNc7zWT4rcKhQ6bdg?key=bLVzrIeeji0xOX_OWgzfwxJh)

At - best selected action at time t

Qt​(a) = Estimated average reward of action a at time t.

Nt(a) = Number of times action aaa has been selected up to time t.

t = Current time step (total number of decisions made so far).

c - trade-off constant (often sqrt(2))

Breaking it down:

First term - Qt(a) (Exploitation term)

- This is the estimated reward (or average observed reward) for action a at time step t.
    
- A higher Qt(a) means action a has historically given good rewards, so it's more likely to be chosen.
    

Second term - Exploration bonus: 

- Nt(a) is the number of times action a has been chosen.
    
- This term encourages trying actions that haven’t been selected frequently.
    
- If Nt(a) is small, the exploration bonus is large, meaning we explore more early on.
    
- As Nt(a) increases, the bonus shrinks, meaning we eventually rely more on Qt(a) and exploit what we’ve learned.
    
- The logarithmic factor ln⁡t ensures that exploration never completely stops, but it decreases over time.
    

  
  

Exception: Thompson Sampling (the method standing out in how it approaches the tradeoff – in the “natural”, e,g, integrated and adaptive, manner)

- Naturally balances exploration and exploitation without explicit tradeoff

- Uses Bayesian approach, choosing actions based on probability of being optimal

- Why it's an exception:

- No explicit exploration rate setting
    
- Adapts exploration-exploitation balance naturally based on the current uncertainty about action values
    
- Explores more when uncertain, exploits more when confident
    
- TS can be seen as a special case of Bayesian Optimization in discrete action spaces
    

  

Relevant theory:

- Theorem 29: Greedy strategies risk not converging to optimal solution

- Theorem 30: Strategies with non-zero probability for all actions guarantee convergence

**