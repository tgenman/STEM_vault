---
aliases: 
publish: true
created: 2024-12-19 13:19
parent: 
connected:
  - "#обс/linking"
---

[[Toward generalizable learning of all (linear) first-order methods via memory augmented Transformers]]







- [[In-Context Learning  (ICL)]]


[[Transformer Architecture]]
[[Memformer]]
	[[Memory replay backpropagation (MRBP)]]
[[Recurrent Memory Transformer]]
[[Memorizing Transformer]]


- [[Gradient-Based Methods]]
	- [[First-Order Optimization Methods]]
		- [[Conjugate Gradient Descent (CGD)]]
		- [[Momentum methods]]
	- [[Higher-Order Optimization Methods]]
		- [[Newtons method]]



**TL;DR**: Memformer и Memorizing Transformer - это две различные архитектуры для улучшения работы с памятью в трансформерах, но они используют разные подходы: Memformer фокусируется на гибкой памяти с динамическим обновлением, а Memorizing Transformer использует внешнюю память для точного запоминания.

Давайте разберем основные различия пошагово:

1. Архитектура памяти:
- Memformer использует многоуровневую память с разными временными масштабами
- Memorizing Transformer применяет единую внешнюю память в виде key-value хранилища

2. Механизм обновления:
- Memformer динамически обновляет память во время обучения и вывода
- Memorizing Transformer хранит точные копии предыдущих входных данных

3. Масштабируемость:
- Memformer более эффективен по памяти благодаря сжатому представлению
- Memorizing Transformer требует больше памяти, но обеспечивает точное воспроизведение

4. Применение:
- Memformer лучше подходит для задач, требующих обобщения и адаптации
- Memorizing Transformer эффективнее для задач, где важно точное запоминание деталей

5. Производительность:
- Memformer показывает лучшие результаты на задачах с длинным контекстом
- Memorizing Transformer превосходит в задачах, требующих точного воспроизведения данных

Эти различия делают каждую архитектуру оптимальной для своего специфического набора задач.