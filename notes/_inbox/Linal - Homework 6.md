Linear Algebra  
(Fall 2023)  
Problem Set 6

Topics covered: bilinear and quadratic forms, Euclidean spaces, orthogonal matrices, projections and least squares.

6.1. Let $\alpha$ be a skew-symmetric bilinear form on an $n$-dimensional vector space $V$ (the ground field $K$ is arbitrary). A basis $\{e_1, \ldots, e_n\}$ of $V$ is called symplectic with respect to $\alpha$ if

$$\alpha(e_{2k-1}, e_{2k}) = -\alpha(e_{2k}, e_{2k-1}) = 1, \quad k = 1, \ldots, m$$

and

$$\alpha(e_i, e_j) = 0 \text{ otherwise.}$$

(for some $m$; it is fairly easy to see that $m = \frac{1}{2}\text{rank}(\alpha)$). Prove the following propositions:

A. (Existence of symplectic bases) For every skew-symmetric $\alpha$ there exists a symplectic basis. \[You may proceed by induction on $n$. For $n > 1$, whenever $\alpha \neq 0$, find vectors $e_1, e_2$ with $\alpha(e_1, e_2) \neq 0$, restrict to $U = \{e_1, e_2\}$, use non-degeneracy of $\alpha$ to obtain $V = U \oplus U^\perp$ and apply the inductive hypothesis.\]

B. The rank of any skew-symmetric bilinear form $\alpha$ is an even number.

6.2. Let $V$ be a finite-dimensional Euclidean vector space with inner product $( \cdot, \cdot )$. For a system of vectors $\{a_1, \ldots, a_k\} \subseteq V$, define the Gram matrix $G(a_1, \ldots, a_k)$ as

$$
G = \begin{bmatrix}
(a_1, a_1) & (a_1, a_2) & \cdots & (a_1, a_k) \\
(a_2, a_1) & (a_2, a_2) & \cdots & (a_2, a_k) \\
\vdots & \vdots & \ddots & \vdots \\
(a_k, a_1) & (a_k, a_2) & \cdots & (a_k, a_k)
\end{bmatrix}.
$$

A. Show that for every system $\{a_1, \ldots, a_k\}$ the inequality

$$\text{det } G(a_1, \ldots, a_k) \geq 0$$

holds, with $\text{det } G(a_1, \ldots, a_k) = 0$ if and only if the system $\{a_1, \ldots, a_k\}$ is linearly dependent.

B. Let $U$ be a subspace of $V$ and let $\{e_1, \ldots, e_k\}$ be an orthonormal basis of $U$. Let $P_U : V \rightarrow U$ be the orthogonal projection onto $U$. Prove the following formula:

$$P_U(a) = \sum_{i=1}^k (a, e_i)e_i.$$

C. Denote by $\|a\|$ the length of the vector $a$: $\|a\| = \sqrt{(a, a)}$. Introduce the distance function:

$$d : V \times V \rightarrow \mathbb{R}_{\geq 0}, \quad d(a, b) = \|a - b\|.$$

Verify that $(V, d)$ is a metric space.

6.3. Recall that in a metric space the distance between a point $a$ and a subset $U$ is given by

$$d(a, U) = \inf_{u \in U} d(a, u).$$

Let $U = \{u_1, \ldots, u_k\}$ be a subspace of $V$ generated by a basis $\{u_1, \ldots, u_k\}$. Prove that

$$d(a, U)^2 = \frac{\text{det } G(u_1, \ldots, u_k, a)}{\text{det } G(u_1, \ldots, u_k)}.$$

6.4. Suppose $A$ is a rectangular real matrix with independent columns, $Q$ is a matrix with orthonormal columns, $R$ is an upper triangular invertible matrix and $A = QR$. Let $b$ be a vector from $\mathbb{R}^m$ and let $p \in C(A)$ be the orthogonal projection[1] of $b$ onto the column space $C(A)$. As a linear combination of the columns of $A$, $p$ has the form $A\hat{x}$. Find the equations that determine $p$ and $\hat{x}$ from $Q, R$ and $b$.

6.5. (An example from Pursell and Trimble, Amer. Math. Mon. Vol. 98, No. 6 (1991), 544-549.) Use Gram-Schmidt to obtain an orthonormal system from the following system of vectors

$$
\begin{aligned}
v_1 &= \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \quad
v_2 &= \begin{bmatrix} -2 \\ 0 \\ 1 \end{bmatrix}, \quad
v_3 &= \begin{bmatrix} 1 \\ 1 \\ 5 \end{bmatrix}.
\end{aligned}
$$

Now consider the $4 \times 3$ matrix $A = [v_1 \quad v_2 \quad v_3]$. Apply row reduction, restricted to be executed exclusively by "add a multiple of a row" transformations, to the augmented matrix

$$
\begin{bmatrix}
A^TA | A^T
\end{bmatrix}.
$$

What does the right-hand side of the resulting row echelon matrix look like? What are the pivots on the left-hand side? Can you think of an explanation?

[1]: This symbol denotes orthogonal projection.
