---
aliases: 
anki: false
created: 2025-02-09 22:37
parent: 
connected:
  - "#обс/linking"
tags:
  - fix/empty
  - fix/study_group
---
[[Word Embeddings]]. [[Word2Vec]]: CBOW, skip-gram, negative sampling. Word2vec properties.

**
Word embeddings (векторные представления слов) - это способ представления слов в виде плотных векторов в непрерывном векторном пространстве. В отличие от традиционных методов, таких как Bag of Words (BoW) или TF-IDF, word embeddings способны улавливать семантическое значение слов и их взаимосвязи, обучаясь на больших текстовых корпусах. Слова со схожими значениями или контекстами отображаются в близкие точки векторного пространства.

Word2Vec - одна из самых влиятельных техник для создания word embeddings. Она была представлена Томашем Миколовым и его коллегами из Google в 2013 году и предлагает две основные модели: Continuous Bag of Words (CBOW) и Skip-Gram.

## Модели Word2Vec

### Continuous Bag of Words (CBOW)

- Цель: предсказать целевое слово по его контекстным словам
- Вход: контекстные слова (окружающие слова в фиксированном окне)
- Выход: целевое слово
- Архитектура:
  - Использует нейронную сеть прямого распространения
  - Контекстные слова преобразуются в embeddings, усредняются и проходят через слой softmax для предсказания целевого слова
- Пример:
  - Предложение: "The cat sat on the mat"
  - Цель: "sat"
  - Контекст: ["the", "cat", "on", "the"]
- Особенность: вычислительно эффективна благодаря усреднению embeddings контекстных слов

### Skip-Gram

- Цель: предсказать контекстные слова по целевому слову
- Вход: целевое слово
- Выход: контекстные слова
- Архитектура:
  - Максимизирует вероятность появления окружающих слов в заданном окне для данного целевого слова
- Пример:
  - Предложение: "The cat sat on the mat"
  - Цель: "sat"
  - Контекст: ["the", "cat", "on", "the"]
- Особенность: хорошо работает с редкими словами и способна улавливать сложные взаимосвязи

## Negative Sampling

Negative sampling - это метод оптимизации вычислительной эффективности путем аппроксимации функции softmax в Word2Vec.

- Проблема softmax:
  - При большом словаре вычисление полной вероятности через softmax требует много ресурсов
- Решение:
  - Использование небольшого числа "отрицательных примеров" (слов, не связанных с целевым словом в контексте)
- Процесс:
  - Выбор отрицательных примеров (случайных слов) из словаря
  - Обучение модели различать истинные контекстные слова и отрицательные примеры
- Преимущество: снижает вычислительную сложность с O(|V|) до O(K), где K - число отрицательных примеров

## Свойства Word2Vec Embeddings

### Семантические и синтаксические отношения
- Улавливает как семантические (например, "король" похож на "королеву"), так и синтаксические (например, времена глаголов) отношения
- Известный пример: король - мужчина + женщина ≈ королева

### Непрерывное векторное пространство
- Embeddings являются плотными (низкоразмерными) по сравнению с BoW или TF-IDF

### Контекстная схожесть
- Слова, встречающиеся в похожих контекстах, имеют близкие embeddings (например, "доктор" и "медсестра")

### Обучение без учителя
- Word2Vec учится напрямую из необработанного текста без размеченных данных

### Снижение размерности
- Слова обычно представляются в пространстве 100-300 измерений, что значительно меньше размера словаря

## Преимущества Word2Vec

- Эффективное представление: компактные векторы, захватывающие значимые паттерны
- Семантическая близость: похожие слова находятся рядом в векторном пространстве
- Масштабируемость: эффективная обработка больших корпусов текста
- Предобученные модели: возможность использования готовых embeddings

## Ограничения Word2Vec

- Независимость от контекста: одно и то же слово всегда имеет одинаковый вектор
- Зависимость от данных: качество зависит от обучающего корпуса
- Редкие слова: плохо работает со словами, редко встречающимися в данных
- Выбор размерности: требует экспериментального подбора

## Применения Word2Vec

- Классификация текстов (анализ тональности, обнаружение спама)
- Кластеризация (группировка похожих слов или документов)
- Машинный перевод
- Информационный поиск
- Рекомендательные системы

## Итог

Word2Vec - это важнейший этап в развитии NLP, позволивший получать содержательные векторные представления слов. Метод предлагает две основные модели (CBOW и Skip-Gram) и использует negative sampling для ускорения обучения. Word2Vec embeddings стали основой для многих NLP-приложений и вдохновили создание более продвинутых методов, таких как GloVe и модели на основе трансформеров (например, BERT).