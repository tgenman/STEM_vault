
### 1. Стратегии Fine-Tuning

Существуют различные стратегии fine-tuning для LLM в зависимости от доступных ресурсов, сложности задачи и желаемой производительности.

### Full Fine-Tuning

**Определение**: Обновление всех параметров предварительно обученной LLM на размеченном наборе данных для конкретной целевой задачи.

**Процесс**:
- Добавление слоя вывода для конкретной задачи
- Обучение всей модели, включая базовую LLM и добавленный выходной слой

**Преимущества**:
- Полная адаптация модели к задаче
- Максимальная производительность

**Недостатки**: 
- Высокие требования к вычислительным ресурсам
- Риск катастрофического забывания

**Применение**:
- Большие наборы размеченных данных
- Критически важные задачи

### P-Tuning (Prompt Tuning)

**Определение**: Облегченная стратегия, использующая обучаемые непрерывные промпты без изменения основных параметров модели.

**Процесс**:
- Внедрение обучаемых prompt embeddings
- Оптимизация только embeddings при замороженной модели
- Направление модели на генерацию через обучаемые промпты

**Преимущества**:
- Эффективность по параметрам
- Хорошая масштабируемость
- Подходит для больших LLM

**Недостатки**:
- Производительность ниже full fine-tuning
- Требует тщательной разработки промптов

**Варианты**:
- P-tuning v1: промпты на входном слое
- P-tuning v2: промпты на нескольких слоях

### Adapters

**Определение**: Небольшие обучаемые нейронные сети, добавляемые к каждому слою предобученной модели.

**Процесс**:
- Вставка bottleneck layers в каждый слой модели
- Обучение только параметров адаптера

**Преимущества**:
- Эффективность по параметрам
- Возможность переиспользования
- Сохранение предварительных знаний

**Недостатки**:
- Необходимость модификации архитектуры
- Сниженная производительность

**Популярные реализации**:
- LoRA: low-rank обновления весов
- Prefix Tuning: модификация входов для механизмов внимания

### Сравнительная таблица

| Стратегия | Параметры | Преимущества | Недостатки | Применение |
|-----------|-----------|--------------|------------|------------|
| Full Fine-Tuning | Все | Высокая точность | Ресурсоемкость | Критичные задачи |
| P-Tuning | Промпты | Эффективность | Сложная настройка | Ограниченные ресурсы |
| Adapters | Малые модули | Модульность | Модификация архитектуры | Multi-task learning |

### Практические рекомендации

**Выбор стратегии зависит от**:
- Размера датасета
- Доступных ресурсов
- Требований к производительности

**Общие правила**:
- Большой датасет → Full fine-tuning
- Ограниченные ресурсы → P-tuning/Adapters
- Нужен баланс → Легковесные стратегии
