---
aliases: 
anki: false
created: 2025-02-08 16:20
parent:
  - "[[Text Feature Representation]]"
connected:
  - "#обс/linking"
tags:
  - content/empty
  - группа
---

> [!tip] Bag of Words (BoW)
Это метод [[Text Feature Representation]] в NLP, который преобразует текст в числовые векторы на основе частоты встречаемости слов, игнорируя порядок слов и грамматику. Это позволяет машинным моделям обрабатывать текстовые данные для задач классификации и кластеризации.

### Как работает BoW

#### 1. Создание словаря
- Формируется словарь, содержащий все уникальные слова из корпуса текстов
- Каждому слову в словаре присваивается уникальный индекс

#### 2. Векторизация документов 
- Каждый документ преобразуется в вектор фиксированной длины:
  - Длина равна размеру словаря
  - Каждая позиция представляет слово из словаря
  - Значения показывают частоту встречаемости каждого слова

#### 3. Document-Term Matrix (DTM)
В результате получается матрица, где:
- Каждая строка представляет документ
- Каждый столбец представляет слово из словаря
- Значения показывают частоту слов в документах

Например, для документов:
1. "The cat sat on the mat"
2. "The dog ran on the grass" 
3. "A cat and dog play"

После предобработки и удаления стоп-слов, DTM будет выглядеть так:

$$
\begin{bmatrix} 
\text{cat} & \text{sat} & \text{mat} & \text{dog} & \text{ran} & \text{grass} & \text{play} \\
\hline
1 & 1 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 \\
1 & 0 & 0 & 1 & 0 & 0 & 1
\end{bmatrix}
$$

Каждая строка представляет один документ, а числа показывают количество вхождений каждого слова.

#### 4. Ключевые особенности
- Порядок слов игнорируется
- Учитывается только частота слов
- Грамматика и синтаксис не сохраняются
- Семантические связи между словами не учитываются

### Пример использования BoW

Дан корпус текстов:
1. "I love NLP"
2. "NLP is fun and exciting"

#### Пошаговое создание BoW:

1. Предобработка:
   - Приведение к нижнему регистру: "i love nlp", "nlp is fun and exciting"
   - Удаление стоп-слов (опционально): "love nlp", "nlp fun exciting"

2. Построение словаря:
   - Словарь: {"love", "nlp", "fun", "exciting"}
   - Индексы: {0: "love", 1: "nlp", 2: "fun", 3: "exciting"}

3. Векторизация:
   - Документ 1: [1, 1, 0, 0] ("love" и "nlp" встречаются по одному разу)
   - Документ 2: [0, 1, 1, 1] ("nlp", "fun" и "exciting" встречаются по одному разу)

| Документ | love | nlp | fun | exciting |
|----------|------|-----|-----|----------|
| Док1     |   1  |  1  |  0  |    0     |
| Док2     |   0  |  1  |  1  |    1     |

### Преимущества BoW

1. Простота:
   - Легко реализовать и интерпретировать
   - Хорошо работает для многих задач NLP

2. Независимость:
   - Не требует предположений о структуре или грамматике языка

3. Универсальность:
   - Применим к любым текстовым данным

### Ограничения BoW

1. Потеря контекста:
   - Игнорирование порядка слов и синтаксиса ведет к потере семантического значения

2. Размерность:
   - Размер словаря растет с размером корпуса, что приводит к разреженной матрице высокой размерности

3. Избыточность признаков:
   - Синонимы и связанные слова рассматриваются как независимые признаки

4. Масштабируемость:
   - Вычислительная неэффективность при работе с большими словарями или корпусами
