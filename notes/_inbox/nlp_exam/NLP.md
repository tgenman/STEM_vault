---
aliases:
  - Natural Language Processing
created: 2025-01-19 23:28
parent: "[[004.85 Machine Learning (notion)|ML]]"
connected: 
tags:
  - content/moc
---

[[One-hot Encoding]]
[[Embeddings]]
[[Text Feature Representation]]


### Экзаменационные вопросы
1. [[Bag of Words (BoW)]], [[TF-IDF (Term Frequency-Inverse Document Frequency)|TF-IDF]]
2. [[2. Word embeddings. Word2vec CBOW, skip-gram, negative sampling. Word2vec properties.]]
3. [[3 CNNs in text processing]]
4. [[Seq2seq, encoder-decoder models]]
5. [[Attention mechanism in encoder-decoder models (as of Bahdanau et.al.)]]
6. [[Machine translation. Problem setup, training inference procedures. Quality metrics for MT.]]
7. [[Unsupervised translation approach]]
8. [[Transformer Architecture and Self-Attention Mechanism in Transformers]]
9. [[Transformer architecture blocks structure, layer norm, training procedure (adamW, lr scheduling, label smoothing)]]

10. BERT pretraining. MLM and NSP objectives.

11. BERT finetuning. Transfer learning paradigm. Finetuning strategies.

12. Discriminative transformer models (ROBERTA, ELECTRA, ALBERT)

13. Neural language modeling. Problem setup, key architectures (GPT-based or analogous).

Quality metrics for LM.

1. Transfer learning paradigm. T5, instruction-tuning (T0).

2. Generative pretraining. Large language models. Basic LLM adaptation (zero-shot, few-

shot prompting).


1. Learning from human feedback for LLMs. RLHF. PPO & DPOL


---

[[16. Transfer Learning with LLMs]]

[[18 LLM alignment]]
[[19 LLM Instruction tuning]]
[[20. Merge algorithms for LLM]]
[[21. Basic RAG]]
[[22. Tools for LLM.]]
