---
publish: true
created: 2025-04-16 13:41
doi: https://doi.org/10.48550/arXiv.2310.01082
author: 
tags:
  - paper
---
[Kwangjun Ahn](https://arxiv.org/search/cs?searchtype=author&query=Ahn,+K), [Xiang Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng,+X), [Minhak Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+M), [Chulhee Yun](https://arxiv.org/search/cs?searchtype=author&query=Yun,+C), [Ali Jadbabaie](https://arxiv.org/search/cs?searchtype=author&query=Jadbabaie,+A), [Suvrit Sra](https://arxiv.org/search/cs?searchtype=author&query=Suvrit)

```
@misc{ahn2024linearattentionmaybeneed,
      title={Linear attention is (maybe) all you need (to understand transformer optimization)}, 
      author={Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
      year={2024},
      eprint={2310.01082},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01082}, 
}
```